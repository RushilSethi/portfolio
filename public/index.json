[{"content":"Credentials üîó Certificate üîó Credly Badge üé¨ YouTube Video Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with just 2 months of preparation while working full-time as a software engineer. In this article, I‚Äôll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my üé¨ YouTube channel where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by Stephane Maarek which is available on Udemy. While taking this course, I dumped all of the information available in the course into a Notion page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I‚Äôll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada üá®üá¶ for my MS.\n","permalink":"//localhost:1313/blog/aws-saa-certification/","summary":"\u003ch1 id=\"credentials\"\u003eCredentials\u003c/h1\u003e\n\u003ch3 id=\"-certificate\"\u003eüîó \u003ca href=\"https://drive.google.com/file/d/1NLGxG3-Id7lGUFL-SVhMl7mvWb9GYIxS/view?usp=sharing\"\u003eCertificate\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-credly-badge\"\u003eüîó \u003ca href=\"https://www.credly.com/badges/dfc84bb4-75ab-449f-bdf5-4dc85eb12ad6/public_url\"\u003eCredly Badge\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-youtube-video\"\u003eüé¨ \u003ca href=\"https://youtu.be/uRyIK28NsCI\"\u003eYouTube Video\u003c/a\u003e\u003c/h3\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eI\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with just 2 months of preparation while working full-time as a software engineer. In this article, I‚Äôll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\u003c/p\u003e\n\u003ch1 id=\"what-is-aws-solutions-architect-associate-certification\"\u003eWhat is AWS Solutions Architect Associate Certification\u003c/h1\u003e\n\u003cp\u003eWith the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\u003c/p\u003e","title":"I passed the AWS SAA Certification Exam"},{"content":"Building My First Dev Portfolio I recently built this portfolio using Hugo and the PaperMod theme. Here\u0026rsquo;s a look into my development process and what I learned along the way.\nWhy This Approach? As a developer starting out, I chose to build with Hugo and PaperMod rather than creating from scratch. This decision let me focus on what matters most right now - documenting my projects and learning journey. While I plan to build a custom portfolio in the future, having a clean, functional site up and running helps me showcase my work today.\nThe Development Journey Building this site expanded my technical toolkit in several ways:\nDiving into static site generation Setting up a development environment with Go and Hugo Implementing continuous deployment with GitHub Actions Managing content through markdown and front matter Exploring Hugo\u0026rsquo;s templating system and partial layouts Optimizing for performance and SEO What\u0026rsquo;s Next This site will grow with me as I continue learning. I\u0026rsquo;m excited to:\nAdd more projects as I build them Start writing about my learning experiences Explore new web technologies Share interesting coding challenges I solve Current Projects Check out my projects page to see what I\u0026rsquo;m working on. I\u0026rsquo;ll be updating it regularly with new work and side projects.\nFeel free to connect with me on GitHub or LinkedIn to follow my development journey!\n","permalink":"//localhost:1313/blog/first/","summary":"\u003ch1 id=\"building-my-first-dev-portfolio\"\u003eBuilding My First Dev Portfolio\u003c/h1\u003e\n\u003cp\u003eI recently built this portfolio using Hugo and the PaperMod theme. Here\u0026rsquo;s a look into my development process and what I learned along the way.\u003c/p\u003e\n\u003ch2 id=\"why-this-approach\"\u003eWhy This Approach?\u003c/h2\u003e\n\u003cp\u003eAs a developer starting out, I chose to build with Hugo and PaperMod rather than creating from scratch. This decision let me focus on what matters most right now - documenting my projects and learning journey. While I plan to build a custom portfolio in the future, having a clean, functional site up and running helps me showcase my work today.\u003c/p\u003e","title":"I passed the AWS SAA Certification Exam"},{"content":"Introduction Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\nDataset In this tutorial, we will use the official¬†DLib Dataset¬†which contains¬†6666 images of varying dimensions. Additionally,¬†labels_ibug_300W_train.xml¬†(comes with the dataset) contains the coordinates of¬†68 landmarks for each face. The script below will download the dataset and unzip it in Colab Notebook.\nif not os.path.exists(\u0026#39;/content/ibug_300W_large_face_landmark_dataset\u0026#39;): !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz !tar -xvzf \u0026#39;ibug_300W_large_face_landmark_dataset.tar.gz\u0026#39; !rm -r \u0026#39;ibug_300W_large_face_landmark_dataset.tar.gz\u0026#39; Here is a sample image from the dataset. We can see that the face occupies a very small fraction of the entire image. If we feed the full image to the neural network, it will also process the background (irrelevant information), making it difficult for the model to learn. Therefore, we need to crop the image and feed only the face portion.\nData Preprocessing To prevent the neural network from overfitting the training dataset, we need to randomly transform the dataset. We will apply the following operations to the training and validation dataset:\nSince the face occupies a very small portion of the entire image, crop the image and use only the face for training. Resize the cropped face into a (224x224) image. Randomly change the brightness and saturation of the resized face. Randomly rotate the face after the above three transformations. Convert the image and landmarks into torch tensors and normalize them between [-1, 1]. class Transforms(): def __init__(self): pass def rotate(self, image, landmarks, angle): angle = random.uniform(-angle, +angle) transformation_matrix = torch.tensor([ [+cos(radians(angle)), -sin(radians(angle))], [+sin(radians(angle)), +cos(radians(angle))] ]) image = imutils.rotate(np.array(image), angle) landmarks = landmarks - 0.5 new_landmarks = np.matmul(landmarks, transformation_matrix) new_landmarks = new_landmarks + 0.5 return Image.fromarray(image), new_landmarks def resize(self, image, landmarks, img_size): image = TF.resize(image, img_size) return image, landmarks def color_jitter(self, image, landmarks): color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1) image = color_jitter(image) return image, landmarks def crop_face(self, image, landmarks, crops): left = int(crops[\u0026#39;left\u0026#39;]) top = int(crops[\u0026#39;top\u0026#39;]) width = int(crops[\u0026#39;width\u0026#39;]) height = int(crops[\u0026#39;height\u0026#39;]) image = TF.crop(image, top, left, height, width) img_shape = np.array(image).shape landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]]) landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]]) return image, landmarks def __call__(self, image, landmarks, crops): image = Image.fromarray(image) image, landmarks = self.crop_face(image, landmarks, crops) image, landmarks = self.resize(image, landmarks, (224, 224)) image, landmarks = self.color_jitter(image, landmarks) image, landmarks = self.rotate(image, landmarks, angle=10) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) return image, landmarks Dataset Class Now that we have our transformations ready, let‚Äôs write our dataset class. The¬†labels_ibug_300W_train.xml¬†contains the image path, landmarks and coordinates for the bounding box (for cropping the face).¬†We will store these values in lists to access them easily during training.¬†In this tutorial, the neural network will be trained on grayscale images.\nclass FaceLandmarksDataset(Dataset): def __init__(self, transform=None): tree = ET.parse(\u0026#39;ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml\u0026#39;) root = tree.getroot() self.image_filenames = [] self.landmarks = [] self.crops = [] self.transform = transform self.root_dir = \u0026#39;ibug_300W_large_face_landmark_dataset\u0026#39; for filename in root[2]: self.image_filenames.append(os.path.join(self.root_dir, filename.attrib[\u0026#39;file\u0026#39;])) self.crops.append(filename[0].attrib) landmark = [] for num in range(68): x_coordinate = int(filename[0][num].attrib[\u0026#39;x\u0026#39;]) y_coordinate = int(filename[0][num].attrib[\u0026#39;y\u0026#39;]) landmark.append([x_coordinate, y_coordinate]) self.landmarks.append(landmark) self.landmarks = np.array(self.landmarks).astype(\u0026#39;float32\u0026#39;) assert len(self.image_filenames) == len(self.landmarks) def __len__(self): return len(self.image_filenames) def __getitem__(self, index): image = cv2.imread(self.image_filenames[index], 0) landmarks = self.landmarks[index] if self.transform: image, landmarks = self.transform(image, landmarks, self.crops[index]) landmarks = landmarks - 0.5 return image, landmarks dataset = FaceLandmarksDataset(Transforms()) Note:¬†landmarks = landmarks - 0.5¬†is done to zero-centre the landmarks as zero-centred outputs are easier for the neural network to learn.\nThe output of the dataset after preprocessing will look something like this (landmarks have been plotted on the image).\nNeural Network We will use the ResNet18 as the basic framework. We need to modify the first and last layers to suit our purpose. In the first layer, we will make the input channel count as 1 for the neural network to accept grayscale images. Similarly, in the final layer, the output channel count should equal¬†68 * 2 = 136¬†for the model to predict the (x, y) coordinates of the 68 landmarks for each face.\nclass Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name=\u0026#39;resnet18\u0026#39; self.model=models.resnet18() self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features, num_classes) def forward(self, x): x=self.model(x) return x Training the Neural Network We will use the Mean Squared Error between the predicted landmarks and the true landmarks as the loss function. Keep in mind that the learning rate should be kept low to avoid exploding gradients. The network weights will be saved whenever the validation loss reaches a new minimum value. Train for at least 20 epochs to get the best performance.\nnetwork = Network() network.cuda() criterion = nn.MSELoss() optimizer = optim.Adam(network.parameters(), lr=0.0001) loss_min = np.inf num_epochs = 10 start_time = time.time() for epoch in range(1,num_epochs+1): loss_train = 0 loss_valid = 0 running_loss = 0 network.train() for step in range(1,len(train_loader)+1): images, landmarks = next(iter(train_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # clear all the gradients before calculating them optimizer.zero_grad() # find the loss for the current step loss_train_step = criterion(predictions, landmarks) # calculate the gradients loss_train_step.backward() # update the parameters optimizer.step() loss_train += loss_train_step.item() running_loss = loss_train/step print_overwrite(step, len(train_loader), running_loss, \u0026#39;train\u0026#39;) network.eval() with torch.no_grad(): for step in range(1,len(valid_loader)+1): images, landmarks = next(iter(valid_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # find the loss for the current step loss_valid_step = criterion(predictions, landmarks) loss_valid += loss_valid_step.item() running_loss = loss_valid/step print_overwrite(step, len(valid_loader), running_loss, \u0026#39;valid\u0026#39;) loss_train /= len(train_loader) loss_valid /= len(valid_loader) print(\u0026#39;\\n--------------------------------------------------\u0026#39;) print(\u0026#39;Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}\u0026#39;.format(epoch, loss_train, loss_valid)) print(\u0026#39;--------------------------------------------------\u0026#39;) if loss_valid \u0026lt; loss_min: loss_min = loss_valid torch.save(network.state_dict(), \u0026#39;/content/face_landmarks.pth\u0026#39;) print(\u0026#34;\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\u0026#34;.format(loss_min, epoch, num_epochs)) print(\u0026#39;Model Saved\\n\u0026#39;) print(\u0026#39;Training Complete\u0026#39;) print(\u0026#34;Total Elapsed Time : {} s\u0026#34;.format(time.time()-start_time)) Predict on Unseen Data Use the code snippet below to predict landmarks in unseen images.\nimport time import cv2 import os import numpy as np import matplotlib.pyplot as plt from PIL import Image import imutils import torch import torch.nn as nn from torchvision import models import torchvision.transforms.functional as TF ####################################################################### image_path = \u0026#39;pic.jpg\u0026#39; weights_path = \u0026#39;face_landmarks.pth\u0026#39; frontal_face_cascade_path = \u0026#39;haarcascade_frontalface_default.xml\u0026#39; ####################################################################### class Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name=\u0026#39;resnet18\u0026#39; self.model=models.resnet18(pretrained=False) self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features,num_classes) def forward(self, x): x=self.model(x) return x ####################################################################### face_cascade = cv2.CascadeClassifier(frontal_face_cascade_path) best_network = Network() best_network.load_state_dict(torch.load(weights_path, map_location=torch.device(\u0026#39;cpu\u0026#39;))) best_network.eval() image = cv2.imread(image_path) grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) height, width,_ = image.shape faces = face_cascade.detectMultiScale(grayscale_image, 1.1, 4) all_landmarks = [] for (x, y, w, h) in faces: image = grayscale_image[y:y+h, x:x+w] image = TF.resize(Image.fromarray(image), size=(224, 224)) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) with torch.no_grad(): landmarks = best_network(image.unsqueeze(0)) landmarks = (landmarks.view(68,2).detach().numpy() + 0.5) * np.array([[w, h]]) + np.array([[x, y]]) all_landmarks.append(landmarks) plt.figure() plt.imshow(display_image) for landmarks in all_landmarks: plt.scatter(landmarks[:,0], landmarks[:,1], c = \u0026#39;c\u0026#39;, s = 5) plt.show() ‚ö†Ô∏è The above code snippet will not work in Colab Notebook as some functionality of the OpenCV is not supported in Colab yet. To run the above cell, use your local machine.\nOpenCV Harr Cascade Classifier is used to detect faces in an image. Object detection using Haar Cascades is a machine learning-based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, pedestrians, and many more. In our case, we will be using the face classifier for which you need to download the pre-trained classifier XML file and save it to your working directory.\nDetected faces in the input image are then cropped, resized to (224, 224) and fed to our trained neural network to predict landmarks in them.\nThe predicted landmarks in the cropped faces are then overlayed on top of the original image. The result is the image shown below. Pretty impressive, right!\nSimilarly, landmarks detection on multiple faces:\nHere, you can see that the OpenCV Harr Cascade Classifier has detected multiple faces including a false positive (a fist is predicted as a face). So, the network has plotted some landmarks on that.\nThat‚Äôs all folks! If you made it till here, hats off to you! You just trained your very own neural network to detect face landmarks in any image. Try predicting face landmarks on your webcam feed!!\nColab Notebook The complete code can be found in the interactive Colab Notebook.\n","permalink":"//localhost:1313/blog/face-landmarks-detection/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eEver wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\u003c/p\u003e\n\u003ch1 id=\"dataset\"\u003eDataset\u003c/h1\u003e\n\u003cp\u003eIn this tutorial, we will use the official¬†\u003ca href=\"http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz\"\u003eDLib Dataset\u003c/a\u003e¬†which contains¬†\u003cstrong\u003e6666 images of varying dimensions\u003c/strong\u003e. Additionally,¬†\u003cem\u003elabels_ibug_300W_train.xml\u003c/em\u003e¬†(comes with the dataset) contains the coordinates of¬†\u003cstrong\u003e68 landmarks for each face\u003c/strong\u003e. The script below will download the dataset and unzip it in Colab Notebook.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"üîó View App üîó GitHub Description Mitraz is a social media platform where users can upload images and text, explore content with infinite scrolling, and enjoy a fully responsive design. Built with React (Vite) and Appwrite, it provides a seamless experience for content curation and sharing.\nüõ†Ô∏è Tech Stack Frontend: React.js (Vite) Styling: Tailwind CSS Backend: Appwrite ","permalink":"//localhost:1313/projects/mitraz/","summary":"\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://mitraz.vercel.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/RushilSethi/mitraz\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eMitraz\u003c/strong\u003e is a \u003cstrong\u003esocial media platform\u003c/strong\u003e where users can \u003cstrong\u003eupload images and text\u003c/strong\u003e, explore content with \u003cstrong\u003einfinite scrolling\u003c/strong\u003e, and enjoy a \u003cstrong\u003efully responsive design\u003c/strong\u003e. Built with \u003cstrong\u003eReact (Vite) and Appwrite\u003c/strong\u003e, it provides a seamless experience for content curation and sharing.\u003c/p\u003e\n\u003ch2 id=\"-tech-stack\"\u003eüõ†Ô∏è Tech Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e React.js (Vite)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStyling:\u003c/strong\u003e Tailwind CSS\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBackend:\u003c/strong\u003e Appwrite\u003c/li\u003e\n\u003c/ul\u003e","title":"Mitraz"},{"content":"\nüîó View App üîó GitHub Description Bit Beacon is a cryptocurrency tracking platform that provides real-time market prices, exchange insights, and news aggregation. It features interactive data visualizations using charts for clear trend analysis. With a fully responsive design and a simple interface, users can explore market movements effortlessly.\nüöÄ Upcoming Features Mutual Fund Insights: Track and analyze mutual fund performance alongside crypto assets. (live now) MetaMask Wallet Integration: Seamless blockchain transactions and wallet management. üõ†Ô∏è Tech Stack Frontend: React.js Styling: CSS3, Ant Design, Tailwind CSS, DaisyUI State Management: Redux \u0026amp; Redux Toolkit APIs Used: CoinRanking, CoinGecko, The Guardian, CryptoNews.com, India Mutual Funds(mfapi.in) ","permalink":"//localhost:1313/projects/bit-beacon/","summary":"\u003cp\u003e\u003cimg alt=\"Mutual Fund Insights\" loading=\"lazy\" src=\"/project_covers/bitbeacon-mf.jpg\"\u003e\u003c/p\u003e\n\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://bit-beacon.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/RushilSethi/bit-beacon\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eBit Beacon\u003c/strong\u003e is a \u003cstrong\u003ecryptocurrency tracking platform\u003c/strong\u003e that provides \u003cstrong\u003ereal-time market prices, exchange insights, and news aggregation\u003c/strong\u003e. It features \u003cstrong\u003einteractive data visualizations\u003c/strong\u003e using charts for clear trend analysis. With a fully responsive design and a simple interface, users can explore market movements effortlessly.\u003c/p\u003e\n\u003ch2 id=\"-upcoming-features\"\u003eüöÄ Upcoming Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMutual Fund Insights\u003c/strong\u003e: Track and analyze mutual fund performance alongside crypto assets. \u003cstrong\u003e(live now)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMetaMask Wallet Integration\u003c/strong\u003e: Seamless blockchain transactions and wallet management.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"-tech-stack\"\u003eüõ†Ô∏è Tech Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e React.js\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStyling:\u003c/strong\u003e CSS3, Ant Design, Tailwind CSS, DaisyUI\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eState Management:\u003c/strong\u003e Redux \u0026amp; Redux Toolkit\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAPIs Used:\u003c/strong\u003e CoinRanking, CoinGecko, The Guardian, CryptoNews.com, India Mutual Funds(mfapi.in)\u003c/li\u003e\n\u003c/ul\u003e","title":"Bit Beacon"},{"content":"üîó View App üîó GitHub Description DailyDrizzle is a real-time weather application that allows users to search for current weather conditions manually or through GPS-based location detection. It provides a 3-day forecast, real-time updates on temperature, humidity, wind speed, and AQI, and features an intuitive UI with a responsive design optimized for all devices.\nüõ†Ô∏è Tech Stack Frontend: HTML, Tailwind CSS, JavaScript API: WeatherAPI ","permalink":"//localhost:1313/projects/daily-drizzle/","summary":"\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://daily-drizzle.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/RushilSethi/daily-drizzle\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eDailyDrizzle\u003c/strong\u003e is a \u003cstrong\u003ereal-time weather application\u003c/strong\u003e that allows users to \u003cstrong\u003esearch for current weather conditions\u003c/strong\u003e manually or through \u003cstrong\u003eGPS-based location detection\u003c/strong\u003e. It provides a \u003cstrong\u003e3-day forecast\u003c/strong\u003e, real-time updates on \u003cstrong\u003etemperature, humidity, wind speed, and AQI\u003c/strong\u003e, and features an \u003cstrong\u003eintuitive UI\u003c/strong\u003e with a \u003cstrong\u003eresponsive design\u003c/strong\u003e optimized for all devices.\u003c/p\u003e\n\u003ch2 id=\"-tech-stack\"\u003eüõ†Ô∏è Tech Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e HTML, Tailwind CSS, JavaScript\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAPI:\u003c/strong\u003e WeatherAPI\u003c/li\u003e\n\u003c/ul\u003e","title":"DailyDrizzle"},{"content":"üîó View App üîó GitHub Description Bajao Music Player is an API-based online music player that streams music from a third-party Saavn API. Users can search, play, pause, skip, shuffle songs, and choose between different quality bitrates for better bandwidth management. The player also displays song title, artist, album art, and lyrics for an enhanced listening experience.\nüõ†Ô∏è Tech Stack Frontend: React (JSX), CSS3, Bootstrap API: 3rd Party Saavn API ","permalink":"//localhost:1313/projects/music-bajao/","summary":"\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://music-bajao.vercel.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/RushilSethi/music-bajao\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eBajao Music Player\u003c/strong\u003e is an \u003cstrong\u003eAPI-based online music player\u003c/strong\u003e that streams music from a \u003cstrong\u003ethird-party Saavn API\u003c/strong\u003e. Users can \u003cstrong\u003esearch, play, pause, skip, shuffle songs\u003c/strong\u003e, and choose between different quality bitrates for better bandwidth management. The player also displays \u003cstrong\u003esong title, artist, album art, and lyrics\u003c/strong\u003e for an enhanced listening experience.\u003c/p\u003e\n\u003ch2 id=\"-tech-stack\"\u003eüõ†Ô∏è Tech Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e React (JSX), CSS3, Bootstrap\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAPI:\u003c/strong\u003e 3rd Party Saavn API\u003c/li\u003e\n\u003c/ul\u003e","title":"Bajao Music Player"},{"content":"üîó View App üîó GitHub Description Pikatube is a video-sharing platform where users can watch, upload, and manage videos, create personalized channels, and interact with others through likes and comments. It features a dark-themed UI, secure authentication, and a seamless streaming experience.\nüõ†Ô∏è Tech Stack Frontend: React (Vite), Redux Toolkit, Tailwind CSS Backend: Node.js, Express.js, MongoDB (Atlas), Mongoose Hosting: Vercel (Frontend), Render (Backend), MongoDB Atlas (Database) ","permalink":"//localhost:1313/projects/pikatube/","summary":"\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://pikatube.vercel.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/RushilSethi/pikatube\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ePikatube\u003c/strong\u003e is a \u003cstrong\u003evideo-sharing platform\u003c/strong\u003e where users can \u003cstrong\u003ewatch, upload, and manage videos\u003c/strong\u003e, create personalized \u003cstrong\u003echannels\u003c/strong\u003e, and interact with others through \u003cstrong\u003elikes and comments\u003c/strong\u003e. It features a \u003cstrong\u003edark-themed UI\u003c/strong\u003e, \u003cstrong\u003esecure authentication\u003c/strong\u003e, and a seamless streaming experience.\u003c/p\u003e\n\u003ch2 id=\"-tech-stack\"\u003eüõ†Ô∏è Tech Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e React (Vite), Redux Toolkit, Tailwind CSS\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBackend:\u003c/strong\u003e Node.js, Express.js, MongoDB (Atlas), Mongoose\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHosting:\u003c/strong\u003e Vercel (Frontend), Render (Backend), MongoDB Atlas (Database)\u003c/li\u003e\n\u003c/ul\u003e","title":"Pikatube"},{"content":"Description Conducted research on the future impact of AI, ML, and cybersecurity, providing key insights for policy recommendations. Designed and developed a dynamic Power BI dashboard, integrating economic data from all Indian states and UTs to support policymaking. Analyzed national initiatives like \u0026ldquo;Production Linked Incentive\u0026rdquo; under \u0026ldquo;Atmanirbhar Bharat,\u0026rdquo; assessing their industrial impact and participating in research for the same. Contributed insights for strategic industrial policies and economic development strategies. üìú View Certificate\n","permalink":"//localhost:1313/experience/niti_aayog/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eConducted research on the future impact of AI, ML, and cybersecurity, providing key insights for policy recommendations.\u003c/li\u003e\n\u003cli\u003eDesigned and developed a dynamic Power BI dashboard, integrating economic data from all Indian states and UTs to support policymaking.\u003c/li\u003e\n\u003cli\u003eAnalyzed national initiatives like \u0026ldquo;Production Linked Incentive\u0026rdquo; under \u0026ldquo;Atmanirbhar Bharat,\u0026rdquo; assessing their industrial impact and participating in research for the same.\u003c/li\u003e\n\u003cli\u003eContributed insights for strategic industrial policies and economic development strategies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/experience/Niti_Aayog_Intern_Certificate.pdf\"\u003eView Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Technology Intern ‚Ä¢ Internship"},{"content":"Description Developed the user interface for an internal dashboard, focusing on improving usability and visual design from the previously used iterations using ReactJS as tech stack. Collaborated with the development team to implement feature updates to the UI to adapt the dashboard to other uses as well and to ensure responsive design with minimal UI clutter. üìú View Certificate\n","permalink":"//localhost:1313/experience/viser_techno/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped the user interface for an internal dashboard, focusing on improving usability and visual design from the previously used iterations using ReactJS as tech stack.\u003c/li\u003e\n\u003cli\u003eCollaborated with the development team to implement feature updates to the UI to adapt the dashboard to other uses as well and to ensure responsive design with minimal UI clutter.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/experience/Techno_Viser_Intern_Certificate.pdf\"\u003eView Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Frontend Intern ‚Ä¢ Internship"},{"content":"Description Built some pages for the frontend of an app called DealDog using React Native which aimed at fetching the best deals on products by comparison on various ecommerce platforms. Assisted in making further updates to the UI like creating a complete search page with category based dynamic options and discount or price based filtering as well. Revamped navigation in the app by switching from a single drawer based navigation to a more balanced tab based navigation for major pages and drawer with internal links to inner pages. üìú View Certificate\n","permalink":"//localhost:1313/experience/delta_edge/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eBuilt some pages for the frontend of an app called DealDog using React Native which aimed at fetching the best deals on products by comparison on various ecommerce platforms.\u003c/li\u003e\n\u003cli\u003eAssisted in making further updates to the UI like creating a complete search page with category based dynamic options and discount or price based filtering as well.\u003c/li\u003e\n\u003cli\u003eRevamped navigation in the app by switching from a single drawer based navigation to a more balanced tab based navigation for major pages and drawer with internal links to inner pages.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/experience/Delta_Edge_Intern_Certificate.pdf\"\u003eView Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":" Android Developer Intern ‚Ä¢ Internship"},{"content":"Issuing Organizations: Internshala Trainings, NSDC (Skill India)\nIssued Date: March 2025\nSkills Covered: MERN Stack, Data Structures \u0026amp; Algorithms, Backend \u0026amp; API Development\nüìú View Internshala Certificate\nüìú View NSDC Certificate\n","permalink":"//localhost:1313/certifications/mern_stack_internshala/","summary":"\u003cp\u003e\u003cstrong\u003eIssuing Organizations:\u003c/strong\u003e Internshala Trainings, NSDC (Skill India)\u003cbr\u003e\n\u003cstrong\u003eIssued Date:\u003c/strong\u003e March 2025\u003cbr\u003e\n\u003cstrong\u003eSkills Covered:\u003c/strong\u003e MERN Stack, Data Structures \u0026amp; Algorithms, Backend \u0026amp; API Development\u003c/p\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/certifications/Full_Stack_Development-Certificate_of_Graduation.pdf\"\u003eView Internshala Certificate\u003c/a\u003e\u003c/strong\u003e\u003cbr\u003e\nüìú \u003cstrong\u003e\u003ca href=\"/certifications/CAN_32146440_4138150.pdf\"\u003eView NSDC Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Certification in Full Stack Development"},{"content":"Issuing Organization: Training and Placement Cell, Bhagwan Parshuram Institute of Technology\nIssued Date: 31st August, 2023\nSkills Covered: Python, SQL, Excel\nüìú View Certificate\n","permalink":"//localhost:1313/certifications/data_analytics_bpit/","summary":"\u003cp\u003e\u003cstrong\u003eIssuing Organization:\u003c/strong\u003e Training and Placement Cell, Bhagwan Parshuram Institute of Technology\u003cbr\u003e\n\u003cstrong\u003eIssued Date:\u003c/strong\u003e 31st August, 2023\u003cbr\u003e\n\u003cstrong\u003eSkills Covered:\u003c/strong\u003e Python, SQL, Excel\u003c/p\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/certifications/Data_Analytics_Certificate.pdf\"\u003eView Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Data Analytics"},{"content":"Issuing Organization: Academind by Maximiliam Schwarzuller, Udemy\nIssued Date: 26th October, 2022\nSkills Covered: ReactJS, Redux\nüìú View Certificate\n","permalink":"//localhost:1313/certifications/reactjs_udemy/","summary":"\u003cp\u003e\u003cstrong\u003eIssuing Organization:\u003c/strong\u003e Academind by Maximiliam Schwarzuller, Udemy\u003cbr\u003e\n\u003cstrong\u003eIssued Date:\u003c/strong\u003e 26th October, 2022\u003cbr\u003e\n\u003cstrong\u003eSkills Covered:\u003c/strong\u003e ReactJS, Redux\u003c/p\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/certifications/ReactJS_Course_Udemy\"\u003eView Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"ReactJS"},{"content":"Skills Languages üñ•Ô∏è HTML, JavaScript, C, C++, Java, Python, mySQL Libraries \u0026amp; Frameworks ‚öôÔ∏è ReactJS, Bootstrap, Tailwind CSS, Flutter Tools \u0026amp; Softwares üõ†Ô∏è Git, VS Code, Firebase, MongoDB, Figma, Canva, Power BI, JWT Soft Skills ü§ù Leadership, Teamwork, Collaboration Certifications Certification in Full Stack Development\rCertification in Full Stack Development from Internshala, covering the complete MERN stack, data structures, and algorithms.\nData Analytics\rCertification in Data Analytics covering Python, SQL, and Excel-based analysis.\nReactJS\rCertification in React.js from Udemy, covering modern React development, hooks, and state management.\n","permalink":"//localhost:1313/skills/","summary":"\u003ch2 id=\"skills\"\u003eSkills\u003c/h2\u003e\n\u003ch3 id=\"languages-\"\u003eLanguages üñ•Ô∏è\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHTML, JavaScript, C, C++, Java, Python, mySQL\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"libraries--frameworks-\"\u003eLibraries \u0026amp; Frameworks ‚öôÔ∏è\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eReactJS, Bootstrap, Tailwind CSS, Flutter\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"tools--softwares-\"\u003eTools \u0026amp; Softwares üõ†Ô∏è\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGit, VS Code, Firebase, MongoDB, Figma, Canva, Power BI, JWT\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"soft-skills-\"\u003eSoft Skills ü§ù\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLeadership, Teamwork, Collaboration\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"certifications\"\u003eCertifications\u003c/h2\u003e\n\u003cdiv class=\"certification-grid\"\u003e\r\n    \r\n    \u003cdiv class=\"certification-card\"\u003e\r\n      \u003ca href=\"/certifications/mern_stack_internshala/\"\u003e\r\n        \u003cdiv class=\"certification-content\"\u003e\r\n          \u003ch3\u003eCertification in Full Stack Development\u003c/h3\u003e\r\n          \u003cp\u003eCertification in Full Stack Development from Internshala, covering the complete MERN stack, data structures, and algorithms.\u003c/p\u003e\r\n        \u003c/div\u003e\r\n      \u003c/a\u003e\r\n    \u003c/div\u003e\r\n    \r\n    \u003cdiv class=\"certification-card\"\u003e\r\n      \u003ca href=\"/certifications/data_analytics_bpit/\"\u003e\r\n        \u003cdiv class=\"certification-content\"\u003e\r\n          \u003ch3\u003eData Analytics\u003c/h3\u003e\r\n          \u003cp\u003eCertification in Data Analytics covering Python, SQL, and Excel-based analysis.\u003c/p\u003e","title":"Skills \u0026 Certifications"},{"content":"Credentials üîó Certificate üîó Credly Badge üé¨ YouTube Video Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with just 2 months of preparation while working full-time as a software engineer. In this article, I‚Äôll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my üé¨ YouTube channel where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by Stephane Maarek which is available on Udemy. While taking this course, I dumped all of the information available in the course into a Notion page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I‚Äôll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada üá®üá¶ for my MS.\n","permalink":"//localhost:1313/blog/aws-saa-certification/","summary":"\u003ch1 id=\"credentials\"\u003eCredentials\u003c/h1\u003e\n\u003ch3 id=\"-certificate\"\u003eüîó \u003ca href=\"https://drive.google.com/file/d/1NLGxG3-Id7lGUFL-SVhMl7mvWb9GYIxS/view?usp=sharing\"\u003eCertificate\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-credly-badge\"\u003eüîó \u003ca href=\"https://www.credly.com/badges/dfc84bb4-75ab-449f-bdf5-4dc85eb12ad6/public_url\"\u003eCredly Badge\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-youtube-video\"\u003eüé¨ \u003ca href=\"https://youtu.be/uRyIK28NsCI\"\u003eYouTube Video\u003c/a\u003e\u003c/h3\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eI\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with just 2 months of preparation while working full-time as a software engineer. In this article, I‚Äôll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\u003c/p\u003e\n\u003ch1 id=\"what-is-aws-solutions-architect-associate-certification\"\u003eWhat is AWS Solutions Architect Associate Certification\u003c/h1\u003e\n\u003cp\u003eWith the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\u003c/p\u003e","title":"I passed the AWS SAA Certification Exam"},{"content":"Building My First Dev Portfolio I recently built this portfolio using Hugo and the PaperMod theme. Here\u0026rsquo;s a look into my development process and what I learned along the way.\nWhy This Approach? As a developer starting out, I chose to build with Hugo and PaperMod rather than creating from scratch. This decision let me focus on what matters most right now - documenting my projects and learning journey. While I plan to build a custom portfolio in the future, having a clean, functional site up and running helps me showcase my work today.\nThe Development Journey Building this site expanded my technical toolkit in several ways:\nDiving into static site generation Setting up a development environment with Go and Hugo Implementing continuous deployment with GitHub Actions Managing content through markdown and front matter Exploring Hugo\u0026rsquo;s templating system and partial layouts Optimizing for performance and SEO What\u0026rsquo;s Next This site will grow with me as I continue learning. I\u0026rsquo;m excited to:\nAdd more projects as I build them Start writing about my learning experiences Explore new web technologies Share interesting coding challenges I solve Current Projects Check out my projects page to see what I\u0026rsquo;m working on. I\u0026rsquo;ll be updating it regularly with new work and side projects.\nFeel free to connect with me on GitHub or LinkedIn to follow my development journey!\n","permalink":"//localhost:1313/blog/first/","summary":"\u003ch1 id=\"building-my-first-dev-portfolio\"\u003eBuilding My First Dev Portfolio\u003c/h1\u003e\n\u003cp\u003eI recently built this portfolio using Hugo and the PaperMod theme. Here\u0026rsquo;s a look into my development process and what I learned along the way.\u003c/p\u003e\n\u003ch2 id=\"why-this-approach\"\u003eWhy This Approach?\u003c/h2\u003e\n\u003cp\u003eAs a developer starting out, I chose to build with Hugo and PaperMod rather than creating from scratch. This decision let me focus on what matters most right now - documenting my projects and learning journey. While I plan to build a custom portfolio in the future, having a clean, functional site up and running helps me showcase my work today.\u003c/p\u003e","title":"I passed the AWS SAA Certification Exam"},{"content":"Introduction Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\nDataset In this tutorial, we will use the official¬†DLib Dataset¬†which contains¬†6666 images of varying dimensions. Additionally,¬†labels_ibug_300W_train.xml¬†(comes with the dataset) contains the coordinates of¬†68 landmarks for each face. The script below will download the dataset and unzip it in Colab Notebook.\nif not os.path.exists(\u0026#39;/content/ibug_300W_large_face_landmark_dataset\u0026#39;): !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz !tar -xvzf \u0026#39;ibug_300W_large_face_landmark_dataset.tar.gz\u0026#39; !rm -r \u0026#39;ibug_300W_large_face_landmark_dataset.tar.gz\u0026#39; Here is a sample image from the dataset. We can see that the face occupies a very small fraction of the entire image. If we feed the full image to the neural network, it will also process the background (irrelevant information), making it difficult for the model to learn. Therefore, we need to crop the image and feed only the face portion.\nData Preprocessing To prevent the neural network from overfitting the training dataset, we need to randomly transform the dataset. We will apply the following operations to the training and validation dataset:\nSince the face occupies a very small portion of the entire image, crop the image and use only the face for training. Resize the cropped face into a (224x224) image. Randomly change the brightness and saturation of the resized face. Randomly rotate the face after the above three transformations. Convert the image and landmarks into torch tensors and normalize them between [-1, 1]. class Transforms(): def __init__(self): pass def rotate(self, image, landmarks, angle): angle = random.uniform(-angle, +angle) transformation_matrix = torch.tensor([ [+cos(radians(angle)), -sin(radians(angle))], [+sin(radians(angle)), +cos(radians(angle))] ]) image = imutils.rotate(np.array(image), angle) landmarks = landmarks - 0.5 new_landmarks = np.matmul(landmarks, transformation_matrix) new_landmarks = new_landmarks + 0.5 return Image.fromarray(image), new_landmarks def resize(self, image, landmarks, img_size): image = TF.resize(image, img_size) return image, landmarks def color_jitter(self, image, landmarks): color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1) image = color_jitter(image) return image, landmarks def crop_face(self, image, landmarks, crops): left = int(crops[\u0026#39;left\u0026#39;]) top = int(crops[\u0026#39;top\u0026#39;]) width = int(crops[\u0026#39;width\u0026#39;]) height = int(crops[\u0026#39;height\u0026#39;]) image = TF.crop(image, top, left, height, width) img_shape = np.array(image).shape landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]]) landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]]) return image, landmarks def __call__(self, image, landmarks, crops): image = Image.fromarray(image) image, landmarks = self.crop_face(image, landmarks, crops) image, landmarks = self.resize(image, landmarks, (224, 224)) image, landmarks = self.color_jitter(image, landmarks) image, landmarks = self.rotate(image, landmarks, angle=10) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) return image, landmarks Dataset Class Now that we have our transformations ready, let‚Äôs write our dataset class. The¬†labels_ibug_300W_train.xml¬†contains the image path, landmarks and coordinates for the bounding box (for cropping the face).¬†We will store these values in lists to access them easily during training.¬†In this tutorial, the neural network will be trained on grayscale images.\nclass FaceLandmarksDataset(Dataset): def __init__(self, transform=None): tree = ET.parse(\u0026#39;ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml\u0026#39;) root = tree.getroot() self.image_filenames = [] self.landmarks = [] self.crops = [] self.transform = transform self.root_dir = \u0026#39;ibug_300W_large_face_landmark_dataset\u0026#39; for filename in root[2]: self.image_filenames.append(os.path.join(self.root_dir, filename.attrib[\u0026#39;file\u0026#39;])) self.crops.append(filename[0].attrib) landmark = [] for num in range(68): x_coordinate = int(filename[0][num].attrib[\u0026#39;x\u0026#39;]) y_coordinate = int(filename[0][num].attrib[\u0026#39;y\u0026#39;]) landmark.append([x_coordinate, y_coordinate]) self.landmarks.append(landmark) self.landmarks = np.array(self.landmarks).astype(\u0026#39;float32\u0026#39;) assert len(self.image_filenames) == len(self.landmarks) def __len__(self): return len(self.image_filenames) def __getitem__(self, index): image = cv2.imread(self.image_filenames[index], 0) landmarks = self.landmarks[index] if self.transform: image, landmarks = self.transform(image, landmarks, self.crops[index]) landmarks = landmarks - 0.5 return image, landmarks dataset = FaceLandmarksDataset(Transforms()) Note:¬†landmarks = landmarks - 0.5¬†is done to zero-centre the landmarks as zero-centred outputs are easier for the neural network to learn.\nThe output of the dataset after preprocessing will look something like this (landmarks have been plotted on the image).\nNeural Network We will use the ResNet18 as the basic framework. We need to modify the first and last layers to suit our purpose. In the first layer, we will make the input channel count as 1 for the neural network to accept grayscale images. Similarly, in the final layer, the output channel count should equal¬†68 * 2 = 136¬†for the model to predict the (x, y) coordinates of the 68 landmarks for each face.\nclass Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name=\u0026#39;resnet18\u0026#39; self.model=models.resnet18() self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features, num_classes) def forward(self, x): x=self.model(x) return x Training the Neural Network We will use the Mean Squared Error between the predicted landmarks and the true landmarks as the loss function. Keep in mind that the learning rate should be kept low to avoid exploding gradients. The network weights will be saved whenever the validation loss reaches a new minimum value. Train for at least 20 epochs to get the best performance.\nnetwork = Network() network.cuda() criterion = nn.MSELoss() optimizer = optim.Adam(network.parameters(), lr=0.0001) loss_min = np.inf num_epochs = 10 start_time = time.time() for epoch in range(1,num_epochs+1): loss_train = 0 loss_valid = 0 running_loss = 0 network.train() for step in range(1,len(train_loader)+1): images, landmarks = next(iter(train_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # clear all the gradients before calculating them optimizer.zero_grad() # find the loss for the current step loss_train_step = criterion(predictions, landmarks) # calculate the gradients loss_train_step.backward() # update the parameters optimizer.step() loss_train += loss_train_step.item() running_loss = loss_train/step print_overwrite(step, len(train_loader), running_loss, \u0026#39;train\u0026#39;) network.eval() with torch.no_grad(): for step in range(1,len(valid_loader)+1): images, landmarks = next(iter(valid_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # find the loss for the current step loss_valid_step = criterion(predictions, landmarks) loss_valid += loss_valid_step.item() running_loss = loss_valid/step print_overwrite(step, len(valid_loader), running_loss, \u0026#39;valid\u0026#39;) loss_train /= len(train_loader) loss_valid /= len(valid_loader) print(\u0026#39;\\n--------------------------------------------------\u0026#39;) print(\u0026#39;Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}\u0026#39;.format(epoch, loss_train, loss_valid)) print(\u0026#39;--------------------------------------------------\u0026#39;) if loss_valid \u0026lt; loss_min: loss_min = loss_valid torch.save(network.state_dict(), \u0026#39;/content/face_landmarks.pth\u0026#39;) print(\u0026#34;\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\u0026#34;.format(loss_min, epoch, num_epochs)) print(\u0026#39;Model Saved\\n\u0026#39;) print(\u0026#39;Training Complete\u0026#39;) print(\u0026#34;Total Elapsed Time : {} s\u0026#34;.format(time.time()-start_time)) Predict on Unseen Data Use the code snippet below to predict landmarks in unseen images.\nimport time import cv2 import os import numpy as np import matplotlib.pyplot as plt from PIL import Image import imutils import torch import torch.nn as nn from torchvision import models import torchvision.transforms.functional as TF ####################################################################### image_path = \u0026#39;pic.jpg\u0026#39; weights_path = \u0026#39;face_landmarks.pth\u0026#39; frontal_face_cascade_path = \u0026#39;haarcascade_frontalface_default.xml\u0026#39; ####################################################################### class Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name=\u0026#39;resnet18\u0026#39; self.model=models.resnet18(pretrained=False) self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features,num_classes) def forward(self, x): x=self.model(x) return x ####################################################################### face_cascade = cv2.CascadeClassifier(frontal_face_cascade_path) best_network = Network() best_network.load_state_dict(torch.load(weights_path, map_location=torch.device(\u0026#39;cpu\u0026#39;))) best_network.eval() image = cv2.imread(image_path) grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) height, width,_ = image.shape faces = face_cascade.detectMultiScale(grayscale_image, 1.1, 4) all_landmarks = [] for (x, y, w, h) in faces: image = grayscale_image[y:y+h, x:x+w] image = TF.resize(Image.fromarray(image), size=(224, 224)) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) with torch.no_grad(): landmarks = best_network(image.unsqueeze(0)) landmarks = (landmarks.view(68,2).detach().numpy() + 0.5) * np.array([[w, h]]) + np.array([[x, y]]) all_landmarks.append(landmarks) plt.figure() plt.imshow(display_image) for landmarks in all_landmarks: plt.scatter(landmarks[:,0], landmarks[:,1], c = \u0026#39;c\u0026#39;, s = 5) plt.show() ‚ö†Ô∏è The above code snippet will not work in Colab Notebook as some functionality of the OpenCV is not supported in Colab yet. To run the above cell, use your local machine.\nOpenCV Harr Cascade Classifier is used to detect faces in an image. Object detection using Haar Cascades is a machine learning-based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, pedestrians, and many more. In our case, we will be using the face classifier for which you need to download the pre-trained classifier XML file and save it to your working directory.\nDetected faces in the input image are then cropped, resized to (224, 224) and fed to our trained neural network to predict landmarks in them.\nThe predicted landmarks in the cropped faces are then overlayed on top of the original image. The result is the image shown below. Pretty impressive, right!\nSimilarly, landmarks detection on multiple faces:\nHere, you can see that the OpenCV Harr Cascade Classifier has detected multiple faces including a false positive (a fist is predicted as a face). So, the network has plotted some landmarks on that.\nThat‚Äôs all folks! If you made it till here, hats off to you! You just trained your very own neural network to detect face landmarks in any image. Try predicting face landmarks on your webcam feed!!\nColab Notebook The complete code can be found in the interactive Colab Notebook.\n","permalink":"//localhost:1313/blog/face-landmarks-detection/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eEver wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\u003c/p\u003e\n\u003ch1 id=\"dataset\"\u003eDataset\u003c/h1\u003e\n\u003cp\u003eIn this tutorial, we will use the official¬†\u003ca href=\"http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz\"\u003eDLib Dataset\u003c/a\u003e¬†which contains¬†\u003cstrong\u003e6666 images of varying dimensions\u003c/strong\u003e. Additionally,¬†\u003cem\u003elabels_ibug_300W_train.xml\u003c/em\u003e¬†(comes with the dataset) contains the coordinates of¬†\u003cstrong\u003e68 landmarks for each face\u003c/strong\u003e. The script below will download the dataset and unzip it in Colab Notebook.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"üîó View App üîó GitHub Description Pikatube is a video-sharing platform where users can watch, upload, and manage videos, create personalized channels, and interact with others through likes and comments. It features a dark-themed UI, secure authentication, and a seamless streaming experience.\nüõ†Ô∏è Tech Stack Frontend: React (Vite), Redux Toolkit, Tailwind CSS Backend: Node.js, Express.js, MongoDB (Atlas), Mongoose Hosting: Vercel (Frontend), Render (Backend), MongoDB Atlas (Database) ","permalink":"//localhost:1313/projects/pikatube/","summary":"\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://pikatube.vercel.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/RushilSethi/pikatube\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ePikatube\u003c/strong\u003e is a \u003cstrong\u003evideo-sharing platform\u003c/strong\u003e where users can \u003cstrong\u003ewatch, upload, and manage videos\u003c/strong\u003e, create personalized \u003cstrong\u003echannels\u003c/strong\u003e, and interact with others through \u003cstrong\u003elikes and comments\u003c/strong\u003e. It features a \u003cstrong\u003edark-themed UI\u003c/strong\u003e, \u003cstrong\u003esecure authentication\u003c/strong\u003e, and a seamless streaming experience.\u003c/p\u003e\n\u003ch2 id=\"-tech-stack\"\u003eüõ†Ô∏è Tech Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e React (Vite), Redux Toolkit, Tailwind CSS\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBackend:\u003c/strong\u003e Node.js, Express.js, MongoDB (Atlas), Mongoose\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHosting:\u003c/strong\u003e Vercel (Frontend), Render (Backend), MongoDB Atlas (Database)\u003c/li\u003e\n\u003c/ul\u003e","title":"Pikatube"},{"content":"\nüîó View App üîó GitHub Description Bit Beacon is a finance tracking platform that provides real-time cryptocurrency and mutual fund data, market insights, and news aggregation. It features interactive data visualizations using charts for clear trend analysis. With a fully responsive design and a simple interface, users can explore both crypto and mutual fund markets effortlessly.\nüöÄ Key Features Crypto Market Tracking: Live prices, exchange insights, and historical trends. Mutual Fund Insights: Track and analyze mutual fund performance. (live now) News Aggregation: Stay updated with the latest crypto and finance news. Upcoming: MetaMask Wallet Integration for seamless blockchain transactions. üõ†Ô∏è Tech Stack Frontend: React.js Styling: CSS3, Ant Design, Tailwind CSS, DaisyUI State Management: Redux \u0026amp; Redux Toolkit APIs Used: CoinRanking, CoinGecko, The Guardian, CryptoNews.com, India Mutual Funds (mfapi.in) ","permalink":"//localhost:1313/projects/bit-beacon/","summary":"\u003cp\u003e\u003cimg alt=\"Mutual Fund Insights\" loading=\"lazy\" src=\"/project_covers/bitbeacon-mf.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://bit-beacon.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/RushilSethi/bit-beacon\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eBit Beacon\u003c/strong\u003e is a \u003cstrong\u003efinance tracking platform\u003c/strong\u003e that provides \u003cstrong\u003ereal-time cryptocurrency and mutual fund data, market insights, and news aggregation\u003c/strong\u003e. It features \u003cstrong\u003einteractive data visualizations\u003c/strong\u003e using charts for clear trend analysis. With a fully responsive design and a simple interface, users can explore both crypto and mutual fund markets effortlessly.\u003c/p\u003e\n\u003ch2 id=\"-key-features\"\u003eüöÄ Key Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCrypto Market Tracking\u003c/strong\u003e: Live prices, exchange insights, and historical trends.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMutual Fund Insights\u003c/strong\u003e: Track and analyze mutual fund performance. \u003cstrong\u003e(live now)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNews Aggregation\u003c/strong\u003e: Stay updated with the latest crypto and finance news.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUpcoming:\u003c/strong\u003e \u003cstrong\u003eMetaMask Wallet Integration\u003c/strong\u003e for seamless blockchain transactions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"-tech-stack\"\u003eüõ†Ô∏è Tech Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e React.js\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStyling:\u003c/strong\u003e CSS3, Ant Design, Tailwind CSS, DaisyUI\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eState Management:\u003c/strong\u003e Redux \u0026amp; Redux Toolkit\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAPIs Used:\u003c/strong\u003e CoinRanking, CoinGecko, The Guardian, CryptoNews.com, India Mutual Funds (mfapi.in)\u003c/li\u003e\n\u003c/ul\u003e","title":"Bit Beacon"},{"content":"üîó View App üîó GitHub Description Bajao Music Player is an API-based online music player that streams music from a third-party Saavn API. Users can search, play, pause, skip, shuffle songs, and choose between different quality bitrates for better bandwidth management. The player also displays song title, artist, album art, and lyrics for an enhanced listening experience.\nüõ†Ô∏è Tech Stack Frontend: React (JSX), CSS3, Bootstrap API: 3rd Party Saavn API ","permalink":"//localhost:1313/projects/music-bajao/","summary":"\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://music-bajao.vercel.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/RushilSethi/music-bajao\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eBajao Music Player\u003c/strong\u003e is an \u003cstrong\u003eAPI-based online music player\u003c/strong\u003e that streams music from a \u003cstrong\u003ethird-party Saavn API\u003c/strong\u003e. Users can \u003cstrong\u003esearch, play, pause, skip, shuffle songs\u003c/strong\u003e, and choose between different quality bitrates for better bandwidth management. The player also displays \u003cstrong\u003esong title, artist, album art, and lyrics\u003c/strong\u003e for an enhanced listening experience.\u003c/p\u003e\n\u003ch2 id=\"-tech-stack\"\u003eüõ†Ô∏è Tech Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e React (JSX), CSS3, Bootstrap\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAPI:\u003c/strong\u003e 3rd Party Saavn API\u003c/li\u003e\n\u003c/ul\u003e","title":"Bajao Music Player"},{"content":"üîó View App üîó GitHub Description Mitraz is a social media platform where users can upload images and text, explore content with infinite scrolling, and enjoy a fully responsive design. Built with React (Vite) and Appwrite, it provides a seamless experience for content curation and sharing.\nüõ†Ô∏è Tech Stack Frontend: React.js (Vite) Styling: Tailwind CSS Backend: Appwrite ","permalink":"//localhost:1313/projects/mitraz/","summary":"\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://mitraz.vercel.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/RushilSethi/mitraz\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eMitraz\u003c/strong\u003e is a \u003cstrong\u003esocial media platform\u003c/strong\u003e where users can \u003cstrong\u003eupload images and text\u003c/strong\u003e, explore content with \u003cstrong\u003einfinite scrolling\u003c/strong\u003e, and enjoy a \u003cstrong\u003efully responsive design\u003c/strong\u003e. Built with \u003cstrong\u003eReact (Vite) and Appwrite\u003c/strong\u003e, it provides a seamless experience for content curation and sharing.\u003c/p\u003e\n\u003ch2 id=\"-tech-stack\"\u003eüõ†Ô∏è Tech Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e React.js (Vite)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStyling:\u003c/strong\u003e Tailwind CSS\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBackend:\u003c/strong\u003e Appwrite\u003c/li\u003e\n\u003c/ul\u003e","title":"Mitraz"},{"content":"üîó View App üîó GitHub Description DailyDrizzle is a real-time weather application that allows users to search for current weather conditions manually or through GPS-based location detection. It provides a 3-day forecast, real-time updates on temperature, humidity, wind speed, and AQI, and features an intuitive UI with a responsive design optimized for all devices.\nüõ†Ô∏è Tech Stack Frontend: HTML, Tailwind CSS, JavaScript API: WeatherAPI ","permalink":"//localhost:1313/projects/daily-drizzle/","summary":"\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://daily-drizzle.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/RushilSethi/daily-drizzle\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eDailyDrizzle\u003c/strong\u003e is a \u003cstrong\u003ereal-time weather application\u003c/strong\u003e that allows users to \u003cstrong\u003esearch for current weather conditions\u003c/strong\u003e manually or through \u003cstrong\u003eGPS-based location detection\u003c/strong\u003e. It provides a \u003cstrong\u003e3-day forecast\u003c/strong\u003e, real-time updates on \u003cstrong\u003etemperature, humidity, wind speed, and AQI\u003c/strong\u003e, and features an \u003cstrong\u003eintuitive UI\u003c/strong\u003e with a \u003cstrong\u003eresponsive design\u003c/strong\u003e optimized for all devices.\u003c/p\u003e\n\u003ch2 id=\"-tech-stack\"\u003eüõ†Ô∏è Tech Stack\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFrontend:\u003c/strong\u003e HTML, Tailwind CSS, JavaScript\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAPI:\u003c/strong\u003e WeatherAPI\u003c/li\u003e\n\u003c/ul\u003e","title":"DailyDrizzle"},{"content":"Description Conducted research on the future impact of AI, ML, and cybersecurity, providing key insights for policy recommendations. Designed and developed a dynamic Power BI dashboard, integrating economic data from all Indian states and UTs to support policymaking. Analyzed national initiatives like \u0026ldquo;Production Linked Incentive\u0026rdquo; under \u0026ldquo;Atmanirbhar Bharat,\u0026rdquo; assessing their industrial impact and participating in research for the same. Contributed insights for strategic industrial policies and economic development strategies. üìú View Certificate\n","permalink":"//localhost:1313/experience/niti_aayog/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eConducted research on the future impact of AI, ML, and cybersecurity, providing key insights for policy recommendations.\u003c/li\u003e\n\u003cli\u003eDesigned and developed a dynamic Power BI dashboard, integrating economic data from all Indian states and UTs to support policymaking.\u003c/li\u003e\n\u003cli\u003eAnalyzed national initiatives like \u0026ldquo;Production Linked Incentive\u0026rdquo; under \u0026ldquo;Atmanirbhar Bharat,\u0026rdquo; assessing their industrial impact and participating in research for the same.\u003c/li\u003e\n\u003cli\u003eContributed insights for strategic industrial policies and economic development strategies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/experience/Niti_Aayog_Intern_Certificate.pdf\"\u003eView Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Technology Intern ‚Ä¢ Internship"},{"content":"Description Developed the user interface for an internal dashboard, focusing on improving usability and visual design from the previously used iterations using ReactJS as tech stack. Collaborated with the development team to implement feature updates to the UI to adapt the dashboard to other uses as well and to ensure responsive design with minimal UI clutter. üìú View Certificate\n","permalink":"//localhost:1313/experience/viser_techno/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped the user interface for an internal dashboard, focusing on improving usability and visual design from the previously used iterations using ReactJS as tech stack.\u003c/li\u003e\n\u003cli\u003eCollaborated with the development team to implement feature updates to the UI to adapt the dashboard to other uses as well and to ensure responsive design with minimal UI clutter.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/experience/Techno_Viser_Intern_Certificate.pdf\"\u003eView Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Frontend Intern ‚Ä¢ Internship"},{"content":"Description Built some pages for the frontend of an app called DealDog using React Native which aimed at fetching the best deals on products by comparison on various ecommerce platforms. Assisted in making further updates to the UI like creating a complete search page with category based dynamic options and discount or price based filtering as well. Revamped navigation in the app by switching from a single drawer based navigation to a more balanced tab based navigation for major pages and drawer with internal links to inner pages. üìú View Certificate\n","permalink":"//localhost:1313/experience/delta_edge/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eBuilt some pages for the frontend of an app called DealDog using React Native which aimed at fetching the best deals on products by comparison on various ecommerce platforms.\u003c/li\u003e\n\u003cli\u003eAssisted in making further updates to the UI like creating a complete search page with category based dynamic options and discount or price based filtering as well.\u003c/li\u003e\n\u003cli\u003eRevamped navigation in the app by switching from a single drawer based navigation to a more balanced tab based navigation for major pages and drawer with internal links to inner pages.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/experience/Delta_Edge_Intern_Certificate.pdf\"\u003eView Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":" Android Developer Intern ‚Ä¢ Internship"},{"content":"Issuing Organizations: Internshala Trainings, NSDC (Skill India)\nIssued Date: March 2025\nSkills Covered: MERN Stack, Data Structures \u0026amp; Algorithms, Backend \u0026amp; API Development\nüìú View Internshala Certificate\nüìú View NSDC Certificate\n","permalink":"//localhost:1313/certifications/mern_stack_internshala/","summary":"\u003cp\u003e\u003cstrong\u003eIssuing Organizations:\u003c/strong\u003e Internshala Trainings, NSDC (Skill India)\u003cbr\u003e\n\u003cstrong\u003eIssued Date:\u003c/strong\u003e March 2025\u003cbr\u003e\n\u003cstrong\u003eSkills Covered:\u003c/strong\u003e MERN Stack, Data Structures \u0026amp; Algorithms, Backend \u0026amp; API Development\u003c/p\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/certifications/Full_Stack_Development-Certificate_of_Graduation.pdf\"\u003eView Internshala Certificate\u003c/a\u003e\u003c/strong\u003e\u003cbr\u003e\nüìú \u003cstrong\u003e\u003ca href=\"/certifications/CAN_32146440_4138150.pdf\"\u003eView NSDC Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Certification in Full Stack Development"},{"content":"Issuing Organization: Training and Placement Cell, Bhagwan Parshuram Institute of Technology\nIssued Date: 31st August, 2023\nSkills Covered: Python, SQL, Excel\nüìú View Certificate\n","permalink":"//localhost:1313/certifications/data_analytics_bpit/","summary":"\u003cp\u003e\u003cstrong\u003eIssuing Organization:\u003c/strong\u003e Training and Placement Cell, Bhagwan Parshuram Institute of Technology\u003cbr\u003e\n\u003cstrong\u003eIssued Date:\u003c/strong\u003e 31st August, 2023\u003cbr\u003e\n\u003cstrong\u003eSkills Covered:\u003c/strong\u003e Python, SQL, Excel\u003c/p\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/certifications/Data_Analytics_Certificate.pdf\"\u003eView Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Data Analytics"},{"content":"Issuing Organization: Academind by Maximiliam Schwarzuller, Udemy\nIssued Date: 26th October, 2022\nSkills Covered: ReactJS, Redux\nüìú View Certificate\n","permalink":"//localhost:1313/certifications/reactjs_udemy/","summary":"\u003cp\u003e\u003cstrong\u003eIssuing Organization:\u003c/strong\u003e Academind by Maximiliam Schwarzuller, Udemy\u003cbr\u003e\n\u003cstrong\u003eIssued Date:\u003c/strong\u003e 26th October, 2022\u003cbr\u003e\n\u003cstrong\u003eSkills Covered:\u003c/strong\u003e ReactJS, Redux\u003c/p\u003e\n\u003cp\u003eüìú \u003cstrong\u003e\u003ca href=\"/certifications/ReactJS_Course_Udemy\"\u003eView Certificate\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"ReactJS"},{"content":"Skills Languages üñ•Ô∏è HTML, JavaScript, C, C++, Java, Python, mySQL Libraries \u0026amp; Frameworks ‚öôÔ∏è ReactJS, Bootstrap, Tailwind CSS, Flutter Tools \u0026amp; Softwares üõ†Ô∏è Git, VS Code, Firebase, MongoDB, Figma, Canva, Power BI, JWT Soft Skills ü§ù Leadership, Teamwork, Collaboration Certifications Certification in Full Stack Development\rCertification in Full Stack Development from Internshala, covering the complete MERN stack, data structures, and algorithms.\nData Analytics\rCertification in Data Analytics covering Python, SQL, and Excel-based analysis.\nReactJS\rCertification in React.js from Udemy, covering modern React development, hooks, and state management.\n","permalink":"//localhost:1313/skills/","summary":"\u003ch2 id=\"skills\"\u003eSkills\u003c/h2\u003e\n\u003ch3 id=\"languages-\"\u003eLanguages üñ•Ô∏è\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHTML, JavaScript, C, C++, Java, Python, mySQL\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"libraries--frameworks-\"\u003eLibraries \u0026amp; Frameworks ‚öôÔ∏è\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eReactJS, Bootstrap, Tailwind CSS, Flutter\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"tools--softwares-\"\u003eTools \u0026amp; Softwares üõ†Ô∏è\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGit, VS Code, Firebase, MongoDB, Figma, Canva, Power BI, JWT\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"soft-skills-\"\u003eSoft Skills ü§ù\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLeadership, Teamwork, Collaboration\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"certifications\"\u003eCertifications\u003c/h2\u003e\n\u003cdiv class=\"certification-grid\"\u003e\r\n    \r\n    \u003cdiv class=\"certification-card\"\u003e\r\n      \u003ca href=\"/certifications/mern_stack_internshala/\"\u003e\r\n        \u003cdiv class=\"certification-content\"\u003e\r\n          \u003ch3\u003eCertification in Full Stack Development\u003c/h3\u003e\r\n          \u003cp\u003eCertification in Full Stack Development from Internshala, covering the complete MERN stack, data structures, and algorithms.\u003c/p\u003e\r\n        \u003c/div\u003e\r\n      \u003c/a\u003e\r\n    \u003c/div\u003e\r\n    \r\n    \u003cdiv class=\"certification-card\"\u003e\r\n      \u003ca href=\"/certifications/data_analytics_bpit/\"\u003e\r\n        \u003cdiv class=\"certification-content\"\u003e\r\n          \u003ch3\u003eData Analytics\u003c/h3\u003e\r\n          \u003cp\u003eCertification in Data Analytics covering Python, SQL, and Excel-based analysis.\u003c/p\u003e","title":"Skills \u0026 Certifications"}]